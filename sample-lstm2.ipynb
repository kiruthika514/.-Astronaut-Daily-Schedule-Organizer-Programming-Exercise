{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sample-lstm2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiruthika514/.-Astronaut-Daily-Schedule-Organizer-Programming-Exercise/blob/main/sample-lstm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pandas numpy h5py\n"
      ],
      "metadata": {
        "id": "bbTebrpXhLvW",
        "outputId": "ed1932b5-b84d-483e-9950-5b33d9f825ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.12.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "HtYHjm0FuK9X"
      },
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import load,dump\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import *\n",
        "from tensorflow.keras.layers import Add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import pydot\n",
        "from numpy import argmax\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2oQm2dOGvunN",
        "outputId": "dfcd4f65-999e-4deb-d3da-d32a5cdefd0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "eo4QCpfauK9g",
        "outputId": "25ffe469-1432-469b-fa7e-75ece731cefc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split(' ')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "    # load all features\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    #print(all_features)\n",
        "    # filter features\n",
        "    features = {k: all_features[k] for k in dataset}\n",
        "    return features\n",
        "\n",
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(domain, tokenizer, max_length, desc_list, photo):\n",
        "    X1, X2, X3, y = list(), list(), list(), list()\n",
        "    # walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        # split one sequence into multiple X,y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # pad input sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            # encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # store\n",
        "            X1.append(photo)\n",
        "            X2.append(domain)\n",
        "            X3.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return array(X1), array(X2), array(X3), array(y)\n",
        "\n",
        "#data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(domain_features, descriptions, photos, tokenizer, max_length):\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            # retrieve the photo feature\n",
        "            domain = domain_features[key]\n",
        "            photo = photos[key][0]\n",
        "            in_img, in_domain, in_seq, out_word = create_sequences(domain, tokenizer, max_length, desc_list, photo)\n",
        "            yield [[in_img, in_domain, in_seq], out_word]\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = '/content/drive/MyDrive/finalattention/Description/trainuc.txt'\n",
        "train = load_set(filename)\n",
        "#print(sorted(train))\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('/content/drive/MyDrive/finalattention/Description/trainuc.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "#print(train)\n",
        "#all_features = load(open('features_inceptionv3_uc.pkl', 'rb'))\n",
        "#print(all_features)\n",
        "\n",
        "train_features = load_photo_features('/content/drive/MyDrive/finalattention/Features/features_uc_resnet152_updated.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "#dump(tokenizer,open('tokenizer_resnet152.pkl','wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "## determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        ""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 1260\n",
            "Descriptions: train=1260\n",
            "Photos: train=1260\n",
            "Vocabulary Size: 293\n",
            "Description Length: 24\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "o2TJ2Kk4uK9k",
        "outputId": "2118b40f-f79d-43fd-bd00-09a4a3d377e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "features=load(open('/content/drive/MyDrive/finalattention/Features/features_resnet152_newids.pkl','rb'))\n",
        "keys  = features.keys()\n",
        "l = list()\n",
        "for i in train:\n",
        "    if i not in keys:\n",
        "        #print(i)\n",
        "        l.append(i)\n",
        "#print(sorted(l))\n",
        "print(len(l))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "7AWArE9ruK9o"
      },
      "cell_type": "code",
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(domain, model, tokenizer, photo, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        domain1 = np.array([domain])\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([photo,domain1,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(domain, model, descriptions, photos, tokenizer, max_length,filename):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        yhat = generate_desc(domain[key], model, tokenizer, photos[key], max_length)\n",
        "        ex=yhat\n",
        "        a=yhat.split('startseq')\n",
        "        b=a[1].split('endseq')\n",
        "        lines.append('beam_size_1'+'\\t'+key + '\\t' + b[0])\n",
        "        references = [d.split() for d in desc_list]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "        #\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    bleu=np.zeros(4)\n",
        "    # calculate BLEU score\n",
        "    bleu[0]=corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
        "    bleu[1]=corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu[2]=corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))\n",
        "    bleu[3]=corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    print('BLEU-1: %f' % bleu[0])\n",
        "    print('BLEU-2: %f' % bleu[1])\n",
        "    print('BLEU-3: %f' % bleu[2])\n",
        "    print('BLEU-4: %f' % bleu[3])\n",
        "    return bleu"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "outputId": "1f1b5d79-b263-4134-8ab6-ef8c1ba085d5",
        "id": "hvlCi3wHBO05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# load test set\n",
        "filename = '/content/drive/MyDrive/finalattention/Description/testuc.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions('/content/drive/MyDrive/finalattention/Description/testuc.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "#print(test)\n",
        "test_features = load_photo_features('/content/drive/MyDrive/finalattention/Features/features_uc_resnet152_updated.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 420\n",
            "Descriptions: test=420\n",
            "Photos: test=420\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "k9m_DaZguK9x",
        "outputId": "9a24892a-aa3d-4ba6-d0e7-deb67ba2c26d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "features = load(open('/content/drive/MyDrive/finalattention/Features/features_resnet152_newids.pkl','rb'))\n",
        "keys = features.keys()\n",
        "for i in test:\n",
        "    if i not in keys:\n",
        "        print(i)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04_070\n",
            "16_078\n",
            "19_064\n",
            "15_061\n",
            "10_068\n",
            "03_068\n",
            "17_069\n",
            "18_063\n",
            "17_080\n",
            "04_071\n",
            "07_067\n",
            "07_068\n",
            "11_073\n",
            "07_077\n",
            "02_077\n",
            "12_068\n",
            "02_063\n",
            "06_079\n",
            "15_070\n",
            "11_068\n",
            "17_075\n",
            "07_076\n",
            "14_070\n",
            "01_070\n",
            "10_079\n",
            "05_076\n",
            "16_077\n",
            "20_071\n",
            "07_063\n",
            "15_067\n",
            "02_075\n",
            "05_078\n",
            "03_076\n",
            "10_077\n",
            "08_079\n",
            "00_065\n",
            "06_070\n",
            "09_077\n",
            "17_061\n",
            "20_068\n",
            "14_080\n",
            "16_067\n",
            "20_070\n",
            "17_071\n",
            "06_068\n",
            "06_063\n",
            "04_062\n",
            "07_069\n",
            "20_075\n",
            "19_070\n",
            "04_069\n",
            "12_071\n",
            "12_074\n",
            "00_077\n",
            "10_063\n",
            "06_075\n",
            "01_063\n",
            "14_064\n",
            "16_065\n",
            "07_064\n",
            "01_067\n",
            "16_061\n",
            "14_074\n",
            "10_073\n",
            "00_074\n",
            "18_064\n",
            "05_074\n",
            "17_068\n",
            "17_070\n",
            "18_078\n",
            "17_073\n",
            "06_069\n",
            "15_066\n",
            "20_063\n",
            "11_065\n",
            "15_076\n",
            "05_064\n",
            "15_062\n",
            "15_077\n",
            "06_071\n",
            "10_080\n",
            "08_068\n",
            "20_072\n",
            "10_078\n",
            "00_070\n",
            "13_075\n",
            "11_071\n",
            "17_077\n",
            "14_079\n",
            "04_076\n",
            "17_063\n",
            "20_067\n",
            "10_067\n",
            "06_076\n",
            "10_065\n",
            "02_066\n",
            "03_064\n",
            "18_075\n",
            "07_066\n",
            "10_061\n",
            "02_072\n",
            "12_072\n",
            "01_071\n",
            "16_064\n",
            "00_062\n",
            "07_079\n",
            "11_078\n",
            "14_073\n",
            "04_065\n",
            "05_072\n",
            "19_079\n",
            "04_061\n",
            "14_075\n",
            "15_078\n",
            "20_080\n",
            "08_061\n",
            "09_079\n",
            "06_080\n",
            "00_066\n",
            "02_080\n",
            "20_066\n",
            "05_069\n",
            "12_080\n",
            "06_077\n",
            "05_070\n",
            "08_069\n",
            "18_076\n",
            "10_066\n",
            "05_073\n",
            "02_065\n",
            "04_080\n",
            "20_062\n",
            "10_071\n",
            "12_066\n",
            "09_073\n",
            "16_063\n",
            "18_077\n",
            "09_078\n",
            "19_063\n",
            "13_069\n",
            "12_070\n",
            "08_077\n",
            "02_070\n",
            "14_069\n",
            "07_078\n",
            "17_072\n",
            "07_072\n",
            "19_066\n",
            "07_075\n",
            "13_080\n",
            "19_061\n",
            "16_072\n",
            "02_078\n",
            "18_079\n",
            "12_064\n",
            "14_076\n",
            "15_074\n",
            "19_080\n",
            "07_070\n",
            "13_065\n",
            "20_077\n",
            "13_079\n",
            "14_065\n",
            "19_062\n",
            "16_070\n",
            "08_072\n",
            "13_066\n",
            "19_074\n",
            "19_068\n",
            "09_062\n",
            "11_063\n",
            "18_071\n",
            "06_073\n",
            "05_065\n",
            "07_061\n",
            "09_071\n",
            "04_079\n",
            "03_070\n",
            "03_069\n",
            "06_066\n",
            "01_075\n",
            "07_062\n",
            "03_073\n",
            "16_080\n",
            "06_061\n",
            "00_063\n",
            "14_061\n",
            "15_063\n",
            "01_079\n",
            "03_079\n",
            "10_069\n",
            "19_067\n",
            "18_061\n",
            "18_070\n",
            "00_076\n",
            "11_072\n",
            "17_078\n",
            "00_075\n",
            "19_073\n",
            "20_079\n",
            "20_061\n",
            "02_067\n",
            "11_077\n",
            "04_075\n",
            "01_078\n",
            "05_067\n",
            "13_071\n",
            "05_068\n",
            "11_067\n",
            "09_074\n",
            "17_067\n",
            "09_070\n",
            "02_064\n",
            "00_080\n",
            "07_073\n",
            "04_077\n",
            "13_073\n",
            "12_069\n",
            "09_064\n",
            "10_072\n",
            "01_062\n",
            "18_080\n",
            "15_073\n",
            "15_068\n",
            "17_074\n",
            "11_080\n",
            "16_074\n",
            "09_065\n",
            "08_080\n",
            "20_065\n",
            "01_065\n",
            "09_068\n",
            "15_075\n",
            "13_077\n",
            "02_061\n",
            "19_072\n",
            "15_080\n",
            "07_074\n",
            "07_080\n",
            "16_066\n",
            "04_063\n",
            "12_076\n",
            "03_074\n",
            "09_080\n",
            "18_069\n",
            "03_063\n",
            "14_062\n",
            "20_073\n",
            "03_066\n",
            "02_068\n",
            "18_065\n",
            "04_064\n",
            "14_063\n",
            "16_073\n",
            "14_072\n",
            "09_063\n",
            "17_079\n",
            "16_071\n",
            "19_069\n",
            "11_070\n",
            "09_067\n",
            "04_067\n",
            "04_078\n",
            "10_076\n",
            "02_074\n",
            "01_073\n",
            "09_069\n",
            "19_071\n",
            "03_067\n",
            "16_076\n",
            "00_072\n",
            "09_076\n",
            "12_075\n",
            "04_074\n",
            "02_069\n",
            "05_063\n",
            "01_077\n",
            "00_073\n",
            "19_077\n",
            "14_071\n",
            "12_078\n",
            "05_071\n",
            "12_077\n",
            "13_061\n",
            "20_076\n",
            "06_074\n",
            "15_079\n",
            "16_068\n",
            "08_065\n",
            "14_077\n",
            "01_074\n",
            "11_074\n",
            "03_071\n",
            "03_061\n",
            "19_076\n",
            "06_078\n",
            "12_061\n",
            "17_076\n",
            "00_061\n",
            "18_062\n",
            "08_070\n",
            "06_064\n",
            "10_070\n",
            "17_064\n",
            "05_080\n",
            "09_072\n",
            "04_066\n",
            "12_065\n",
            "03_078\n",
            "04_072\n",
            "13_063\n",
            "18_066\n",
            "01_069\n",
            "08_075\n",
            "01_068\n",
            "13_078\n",
            "08_063\n",
            "03_080\n",
            "04_073\n",
            "14_066\n",
            "19_075\n",
            "01_080\n",
            "15_072\n",
            "05_066\n",
            "08_066\n",
            "05_062\n",
            "18_068\n",
            "18_072\n",
            "20_078\n",
            "15_071\n",
            "03_062\n",
            "13_072\n",
            "12_063\n",
            "01_066\n",
            "13_068\n",
            "13_064\n",
            "11_064\n",
            "00_064\n",
            "12_073\n",
            "16_062\n",
            "03_065\n",
            "14_078\n",
            "06_065\n",
            "11_069\n",
            "15_065\n",
            "19_065\n",
            "13_062\n",
            "13_067\n",
            "06_062\n",
            "16_069\n",
            "11_075\n",
            "16_079\n",
            "15_064\n",
            "11_062\n",
            "19_078\n",
            "02_073\n",
            "11_076\n",
            "00_068\n",
            "09_061\n",
            "14_067\n",
            "12_067\n",
            "00_067\n",
            "17_062\n",
            "02_062\n",
            "01_061\n",
            "18_074\n",
            "07_065\n",
            "10_074\n",
            "14_068\n",
            "08_074\n",
            "06_067\n",
            "04_068\n",
            "10_075\n",
            "07_071\n",
            "16_075\n",
            "08_062\n",
            "05_061\n",
            "09_066\n",
            "03_077\n",
            "08_067\n",
            "00_071\n",
            "13_076\n",
            "05_077\n",
            "20_064\n",
            "13_070\n",
            "00_079\n",
            "15_069\n",
            "02_079\n",
            "20_074\n",
            "03_075\n",
            "01_072\n",
            "12_062\n",
            "20_069\n",
            "17_065\n",
            "10_062\n",
            "02_071\n",
            "08_073\n",
            "05_079\n",
            "11_079\n",
            "01_064\n",
            "06_072\n",
            "18_073\n",
            "11_061\n",
            "11_066\n",
            "10_064\n",
            "01_076\n",
            "17_066\n",
            "05_075\n",
            "12_079\n",
            "08_071\n",
            "18_067\n",
            "02_076\n",
            "00_078\n",
            "09_075\n",
            "08_076\n",
            "00_069\n",
            "13_074\n",
            "08_078\n",
            "03_072\n",
            "08_064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "my420c6viid_",
        "outputId": "82c62deb-1ae1-4869-f73c-bd16fb2e9f77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mCOfGXcuuK90"
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
        "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "id": "eY9-yGSXmcWl",
        "outputId": "0e778e7d-6e43-4794-e7fa-1698939aeb40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from pickle import dump\n",
        "\n",
        "# Load the dataset (modify this based on your dataset file)\n",
        "dataset_path = \"/content/drive/MyDrive/finalattention/Description/trainnew.txt\"\n",
        "\n",
        "# Read captions from dataset\n",
        "with open(dataset_path, \"r\") as f:\n",
        "    captions = f.readlines()\n",
        "\n",
        "# Preprocess: Convert to lowercase, split into words\n",
        "sentences = [caption.strip().lower().split() for caption in captions]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the embedding as a dictionary\n",
        "word_vectors = {word: word2vec_model.wv[word] for word in word2vec_model.wv.index_to_key}\n",
        "\n",
        "# Save to a file\n",
        "embedding_path = \"/content/drive/MyDrive/finalattention/Features/word2vec_embedding.pkl\"\n",
        "with open(embedding_path, \"wb\") as f:\n",
        "    dump(word_vectors, f)\n",
        "\n",
        "print(\"✅ Word2Vec Embedding saved successfully!\")\n"
      ],
      "metadata": {
        "id": "DvSyWa1hmIIT",
        "outputId": "059681f7-8e8b-432b-fd80-1b680bd46b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Word2Vec Embedding saved successfully!\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "iZz12TaouK94"
      },
      "cell_type": "code",
      "source": [
        "# load a word embedding\n",
        "def load_embedding(tokenizer, vocab_size, max_length):\n",
        "\t# load the tokenizer\n",
        "\tembedding = load(open('/content/drive/MyDrive/finalattention/Features/word2vec_embedding.pkl', 'rb'))\n",
        "\tdimensions = 100\n",
        "\ttrainable = False\n",
        "\t# create a weight matrix for words in training docs\n",
        "\tweights = np.zeros((vocab_size, dimensions))\n",
        "\t# walk words in order of tokenizer vocab to ensure vectors are in the right index\n",
        "\tfor word, i in tokenizer.word_index.items():\n",
        "\t\tif word not in embedding:\n",
        "\t\t\tcontinue\n",
        "\t\tweights[i] = embedding[word]\n",
        "\tlayer = Embedding(vocab_size, dimensions, weights=[weights], input_length=max_length, trainable=trainable, mask_zero=True)\n",
        "\treturn layer"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path\n",
        "model_dir = \"/content/drive/MyDrive/finalattention/Modelcall\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Now define the model\n",
        "model = define_model(vocab_size, max_length)\n"
      ],
      "metadata": {
        "id": "DAx74Rdzjwox",
        "outputId": "6b9f0256-aaa2-4009-b62c-0657875cfdf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'define_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bcd63348dd4f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Now define the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'define_model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "U4Kjr8zpuK9-"
      },
      "cell_type": "code",
      "source": [
        "# define the captioning model\n",
        "loc = '/content/drive/MyDrive/finalattention/Modelcall/'\n",
        "def define_model(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    #fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(inputs1)\n",
        "\n",
        "    inputs2 = Input(shape=(21,))\n",
        "    merged = concatenate([fe2,inputs2])\n",
        "\n",
        "    fe3 = RepeatVector(max_length)(merged)\n",
        "\n",
        "    # sequence model\n",
        "    inputs3 = Input(shape=(max_length,))\n",
        "    #se1 = load_embedding(tokenizer, vocab_size, max_length)(inputs3)\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs3)\n",
        "    #print(se1.shape)\n",
        "    se2 = LSTM(256,return_sequences=True)(se1)\n",
        "    #print(se2.shape)\n",
        "    #x = Attention(max_length)(se2)\n",
        "    #print(x.shape)\n",
        "    se3 = TimeDistributed(Dense(256,activation='relu'))(se2)\n",
        "    #print(se3.shape)\n",
        "\n",
        "    # decoder model\n",
        "    decoder1 = concatenate([fe3, se3])\n",
        "    decoder2 = Bidirectional(LSTM(150,return_sequences=True))(decoder1)\n",
        "    decoder3 = Attention(max_length)(decoder2)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    # summarize model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file=loc+'model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMFx02ZSuK-C"
      },
      "cell_type": "code",
      "source": [
        "model = define_model(vocab_size, max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "caIH9ThQuK-G"
      },
      "cell_type": "code",
      "source": [
        "embedding = load(open('UCMDataset1/Features/word2vec_embedding.pkl', 'rb'))\n",
        "embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n0LjHWyYuK-L"
      },
      "cell_type": "code",
      "source": [
        "domain_features = load(open('drive/My Drive/Final Year Project/Features/domain_features.pkl', 'rb'))\n",
        "print(domain_features['100_077'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZzUE-B5zuK-P"
      },
      "cell_type": "code",
      "source": [
        "## train the model, run epochs manually and save after each epoch\n",
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "mylist = list()\n",
        "df=pd.DataFrame(index=mylist, columns=['model_no','Bleu_1','Bleu_2','Bleu_3','Bleu_4'])\n",
        "for i in range(1,epochs):\n",
        "    print(i)\n",
        "    # create the data generator\n",
        "    generator = data_generator(domain_features, train_descriptions, train_features, tokenizer, max_length)\n",
        "    # fit for one epoch\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    # save model\n",
        "    model.save(loc+'model_' + str(i) + '.h5')\n",
        "    # load the mode\n",
        "    filename = loc+'result/text_predicted_' + str(i) + '.txt'\n",
        "    # evaluate model\n",
        "    bleu = evaluate_model(domain_features, model, test_descriptions, test_features, tokenizer, max_length,filename)\n",
        "    #bleu = evaluate_model(model, test_descriptions, test_features,tokenizer, max_length,filename_new)\n",
        "    df.at[i,'model_no']=i\n",
        "    df.at[i,'Bleu_1']=bleu[0]\n",
        "    df.at[i,'Bleu_2']=bleu[1]\n",
        "    df.at[i,'Bleu_3']=bleu[2]\n",
        "    df.at[i,'Bleu_4']=bleu[3]\n",
        "    df.to_csv(loc+'result/results.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lo6LI-cTuK-T"
      },
      "cell_type": "code",
      "source": [
        "## from keras.models import load_model\n",
        "filename = 'UCMDataset/text_predicted.txt'\n",
        "model = model.load_weights('Modelnewdataall/model_1.h5')\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length,filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMOASgS6uK-W"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}